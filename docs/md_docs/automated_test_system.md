# FeatFloWer Automated Test System (Backend-First Design)

This document proposes a backend-first automated test system for FeatFloWer that targets
regression testing, performance tracking, and reproducible benchmark runs on
RHEL 9.7 with SLURM. It is designed so a front-end dashboard can be added later
with minimal rework.

The initial test coverage uses the benchmark workflows described in:

- `docs/md_docs/guide_01_q2p1_fc_ext_cylinder_benchmark_from_scratch.md`
- `docs/md_docs/guide_02_q2p1_bench_sedimentation_pe_serial_from_scratch.md`
- `build_guide.md`

## Goals & Constraints

**Goals**
- Fully automated execution of selected benchmarks via SLURM.
- Clear separation of *definition* (what to run) vs *execution* (how to run).
- Store logs, metrics, and artifacts in a structure that a future web dashboard
  can query and visualize.

**Constraints**
- Runs on RHEL 9.7.
- Jobs must be submitted through SLURM (`sbatch`, `sacct`, `squeue`).
- Initial benchmarks are the two guides listed above.

## High-Level Architecture

```
+---------------------+         +------------------------+
| Test Definitions    |         | Scheduler Adapter      |
| (YAML + templates)  |         | (SLURM: sbatch/sacct)  |
+----------+----------+         +-----------+------------+
           |                                |
           v                                v
+---------------------+         +------------------------+
| Orchestrator        |  ---->  | Execution Runner       |
| (CLI + API)         |         | (per-test job scripts) |
+----------+----------+         +-----------+------------+
           |                                |
           v                                v
+---------------------+         +------------------------+
| Results Store       | <-----  | Artifact Collector     |
| (DB + JSON artifacts)|        | (logs, prot.txt, etc.) |
+----------+----------+         +-----------+------------+
           |
           v
+---------------------+
| Dashboard API       |
| (read-only, JSON)   |
+---------------------+
```

### Key Ideas
- **Test definitions** are declarative and versioned with the repo.
- **Runner** generates SLURM job scripts and submits them.
- **Collector** extracts metrics and stores them as structured JSON.
- **Results store** is the source of truth for the future dashboard.

## Test Definition Format (Backend-First)

Use YAML to define a test case with explicit stages. The runner turns each stage
into commands inside a job script. Keep this in-repo under `testcases/` to match
existing conventions.

Example (conceptual):

```yaml
id: q2p1_fc_ext_cylinder
name: Q2P1 FC_EXT Cylinder Benchmark
build:
  cmake:
    build_dir: build-release
    generator: Ninja
    build_type: Release
    options:
      - USE_HYPRE=ON
      - BUILD_APPLICATIONS=ON
      - CMAKE_C_COMPILER=mpicc
      - CMAKE_CXX_COMPILER=mpicxx
      - CMAKE_Fortran_COMPILER=mpifort
  targets:
    - q2p1_fc_ext
    - metis
run:
  workdir: build-release/applications/q2p1_fc_ext
  env:
    Q2P1_MESH_DIR: /path/to/mesh-repo
  partition:
    command: >-
      python3 /path/to/FeatFloWer/tools/PyPartitioner.py
      3 1 1 NEWFAC _adc/2D_FAC/2Dbench.prj
  mpi:
    ranks: 4
    command: ./q2p1_fc_ext
  outputs:
    logs:
      - run_q2p1_fc_ext_np4.log
    artifacts:
      - _data/prot.txt
metrics:
  drag_lift:
    extract:
      command: "grep 'Force acting' _data/prot.txt | tail -1"
    compare:
      type: tolerance
      fields: [drag, lift]
      tolerance: [1.0e-4, 1.0e-4]
```

This model is intentionally *dashboard-friendly*: the same file can be rendered
as a UI view (inputs, build options, command lines, results).

## Initial Test Set (Guide 01 & Guide 02)

### Test 1: q2p1_fc_ext Cylinder Benchmark (Guide 01)
- **Build**: `q2p1_fc_ext` + `metis` with `USE_HYPRE=ON`.
- **Prepare**: `Q2P1_MESH_DIR`, symlink `_adc`, partition with `PyPartitioner`.
- **Run**: `mpirun -np 4 ./q2p1_fc_ext` (3 partitions + 1 rank convention).
- **Metrics**: Extract drag/lift from `_data/prot.txt`.
- **Artifacts**: `run_q2p1_fc_ext_np4.log`, `_data/prot.txt`.

### Test 2: q2p1_bench_sedimentation PE Serial (Guide 02)
- **Build**: `q2p1_bench_sedimentation` + `metis` with
  `USE_PE=ON`, `USE_PE_SERIAL_MODE=ON`, and `USE_JSON=ON`.
- **Prepare**: Partition sedimentation mesh, stage PE JSON config.
- **Run**: MPI launch as in guide (with PE serial mode).
- **Metrics**: Capture `SED_BENCH` outputs (force/pos/vel) if enabled, and
  parse from log or benchmark file.
- **Artifacts**: build logs, runtime logs, JSON config, and any `sed_bench`
  outputs generated by the app.

## SLURM Integration

The runner generates `sbatch` scripts with the following structure:

1. **Module setup** (RHEL 9.7):
   ```bash
   source /etc/profile.d/modules.sh
   module purge
   module load gcc/latest-v13 openmpi/options/interface/ethernet openmpi/4.1.6
   ```
2. **Build stage**: configure + compile.
3. **Run stage**: partition + `mpirun` invocation.
4. **Capture outputs**: redirect to log files; copy to artifacts directory.
5. **Exit status** is recorded and propagated.

Example SLURM settings (to be parameterized per test):

- `--nodes`, `--ntasks`, `--cpus-per-task`, `--time`, `--partition`
- `--output` and `--error` to store in a structured `artifacts/` folder
- Optional `--constraint` or `--account` for site policies

## Result Storage Model

A backend-friendly storage layout should enable later dashboard views:

```
results/
  runs/
    <run-id>/
      metadata.json
      build.log
      run.log
      artifacts/
        prot.txt
        sed_bench.txt
      metrics.json
```

**metadata.json** includes:
- Git commit SHA
- Test ID + name
- Build options
- SLURM job ID(s)
- Start/end timestamps

**metrics.json** stores extracted numbers with comparison status, enabling
trend charts and regression alerts.

For version 1, a **filesystem-based store** is sufficient. The orchestrator can
optionally write a summary row into a lightweight DB (SQLite). When scaling up,
move to PostgreSQL (or TimescaleDB for time-series metrics).

## Comparison & Alerting

- **Regression**: compare key metrics (drag/lift, sedimentation force/velocity)
  against a baseline with tolerance bands.
- **Performance**: track walltime, solver iterations, and key stage timings.
- **Alerting**: flag failed comparisons; record in `metrics.json` for the
  future UI. Optional notifications (email/Slack) can be added later.

## Future Dashboard Considerations

Design now for an easy front-end later:
- Expose a **read-only JSON API** from the results store.
- Keep outputs structured and machine-readable.
- Store metadata and metrics with consistent keys and units.

## Recommended Tech Stack

**Backend (initial):**
- **Language**: Python 3 (ubiquitous on RHEL and already used for tooling).
- **CLI Orchestrator**: `typer` or `click` for a clean interface.
- **YAML parsing**: `pyyaml`.
- **Results store**: filesystem + optional SQLite for quick queries.
- **SLURM integration**: subprocess calls to `sbatch`, `squeue`, `sacct`.
- **Metrics parsing**: `rg`/`grep` pipelines or Python regex.

**Backend (scalable):**
- **API**: FastAPI for a future dashboard service.
- **Database**: PostgreSQL (TimescaleDB if long-term metrics tracking matters).

**Frontend (future):**
- **Dashboard**: React/Next.js (or a light-weight internal view with
  FastAPI + Jinja if minimal UI is preferred).
- **Visualization**: Plotly or ECharts for trend plots, table views, and
  pass/fail status.

## Next Steps

1. Implement a minimal **test definition schema** and a CLI runner.
2. Add a **SLURM submission adapter** that builds job scripts.
3. Add **parsers** for guide 01 and guide 02 metrics.
4. Define a **baseline** for each benchmark and store in `testcases/`.
5. Add a **summary report** (JSON + Markdown) for CI or dashboard ingestion.
